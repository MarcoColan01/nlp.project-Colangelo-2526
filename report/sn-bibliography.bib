@misc{hinton2015distillingknowledgeneuralnetwork,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1503.02531}, 
}

@misc{jiao2020tinybertdistillingbertnatural,
      title={TinyBERT: Distilling BERT for Natural Language Understanding}, 
      author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
      year={2020},
      eprint={1909.10351},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.10351}, 
}

@misc{ji2020knowledgedistillationwideneural,
      title={Knowledge Distillation in Wide Neural Networks: Risk Bound, Data Efficiency and Imperfect Teacher}, 
      author={Guangda Ji and Zhanxing Zhu},
      year={2020},
      eprint={2010.10090},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2010.10090}, 
}